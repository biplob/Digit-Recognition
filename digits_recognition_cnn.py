# -*- coding: utf-8 -*-
"""Digits_Recognition_CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ou5JQCIxWll0hhSv-U2MTo7bi1JaXKd2
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sn
import numpy as np
import pandas as pd
import math
import datetime
import platform

print('Python Version:', platform.python_version())
print('Tensorflow Version:', tf.__version__)
#print('Keras Version:', tf.keras._version_)

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension.
# %reload_ext tensorboard
# %load_ext tensorboard

# Clear any logs from previous runs.
!rm -rf ./.logs/

"""## Load The Data

The **training** dataset consists of 60000 28x28px images of hand-writing digits from '0' to '9'.

The **test** dataset consists of 10000 28x28px images.
"""

mnist_dataset = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist_dataset.load_data()

print('x_train:', x_train.shape)
print('y_train', y_train.shape)
print('x_test:', x_test.shape)
print('y_test:', y_test.shape)

# Save image parameters to the constants that we will use later for data re-shaping and for model traning.
(_, IMAGE_WIDTH, IMAGE_HEIGHT) = x_train.shape
IMAGE_CHANNELS = 1

print('IMAGE_WIDTH:', IMAGE_WIDTH)
print('IMAGE_HEIGHT:', IMAGE_HEIGHT)
print('IMAGE_CHANNELS:', IMAGE_CHANNELS)

"""### Explore The Data

Here is how each image in the dataset looks like. It is a 28x28 matrix of integers (from 0 to 255). Each integer represents a color of a pixcel.
"""

pd.DataFrame(x_train[0])

"""This matrix of numbers drawn as follows:

*   List Iteam
*   List Iteam
"""

plt.imshow(x_train[0], cmap=plt.cm.binary)
plt.show()

plt.imshow(x_train[1], cmap=plt.cm.binary)
plt.show()

plt.imshow(x_train[2], cmap=plt.cm.binary)
plt.show()

"""Let's print some more taining examples to get the feeling of how the digits were
written.
"""

numbers_to_display = 30
num_cells = math.ceil(math.sqrt(numbers_to_display))
plt.figure(figsize=(10,10))
for i in range(numbers_to_display):
  plt.subplot(num_cells, num_cells, i+1)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.imshow(x_train[i], cmap=plt.cm.binary)
  plt.xlabel(y_train[i])

plt.show()

"""### Reshaping the data

In order to use convolution layers we need to reshape our data and add a color channel to it. As you've noticed currently every digit  hase a shape of (28,28) which means that it is a 28x28 matrix of color values from 0 to 255. We need to reshape it it (28, 28, 1) shape so that each pixel potentially may have multiple channels(like Red, Green and Blue).
"""

x_train_with_channels = x_train.reshape(
    x_train.shape[0],
    IMAGE_WIDTH,
    IMAGE_HEIGHT,
    IMAGE_CHANNELS

)

x_test_with_channels = x_test.reshape(
    x_test.shape[0],
    IMAGE_WIDTH,
    IMAGE_HEIGHT,
    IMAGE_CHANNELS
)

print('x_train_with_channels:', x_train_with_channels.shape)
print('x_test_with_channels:', x_test_with_channels.shape)



"""### Normalize the data

Here we're just trying to move from values range of [0....255] to [0...1]
"""

x_train_normalized = x_train_with_channels / 255
x_test_normalized = x_test_with_channels / 255

# Let's check just one row from 0th image to see color channel values after normalization.
x_train_normalized[0][21]

"""### Build the model

We will use **Sequential** Keras model.
Then we will have two pairs of **Convolution2D** and **MaxPooling2D** layers. The MaxPooling layer acts as a sort of downsampling using max values in a region instead of averaging.

After that we will use **Flatten** layer to covert multidimensional parameters to vector.

The las layer will be a **Dense** layer with 10 **Softmax** outputs. The output represents the network guess. The 0-th output represents a probability that the input digit is 0, the 1-st output represents a probability that the input digit is 1 and so on...
"""

model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Convolution2D(
    input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS),
    kernel_size=5,
    filters=8,
    strides=1,
    activation=tf.keras.activations.relu,
    kernel_initializer=tf.keras.initializers.VarianceScaling()
))

model.add(tf.keras.layers.MaxPooling2D(
    pool_size=(2,2),
    strides=(2,2)
))

model.add(tf.keras.layers.Convolution2D(
    kernel_size=5,
    filters=16,
    strides=1,
    activation=tf.keras.activations.relu,
    kernel_initializer=tf.keras.initializers.VarianceScaling()
))

model.add(tf.keras.layers.MaxPooling2D(
    pool_size=(2,2),
    strides=(2,2)
))


model.add(tf.keras.layers.Flatten())

model.add(tf.keras.layers.Dense(
    units=128,
    activation=tf.keras.activations.relu
))

model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(
    units=10,
    activation=tf.keras.activations.softmax,
    kernel_initializer=tf.keras.initializers.VarianceScaling()
))

"""Here is our model summary so far.

"""

model.summary()

"""In order to plot the model the 'Graphviz' should be installed."""

tf.keras.utils.plot_model(
    model,
    show_shapes=True,
    show_layer_names=True
)

"""## Compile the model"""

adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(
    optimizer=adam_optimizer,
    loss=tf.keras.losses.sparse_categorical_crossentropy,
    metrics=['accuracy']
)

"""## Train the model"""

log_dir=".logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

training_history = model.fit(
    x_train_normalized,
    y_train,
    epochs=10,
    validation_data=(x_test_normalized, y_test),
    callbacks=[tensorboard_callback]
)

"""Let's see how the lost function was changing during the training. We expect it to get smaller and smaller on every next epoch"""

plt.xlabel('Epoch Number')
plt.ylabel('Loss')
plt.plot(training_history.history['loss'], label='training set')
plt.plot(training_history.history['val_loss'], label='test set')
plt.legend()

plt.xlabel('Epoch Number')
plt.ylabel('Accuracy')
plt.plot(training_history.history['accuracy'], label='training set')
plt.plot(training_history.history['val_accuracy'], label='test set')
plt.legend()

"""## Evaluate model accuracy

We need to compare the accuracy of our model on **training** set and on **test** set. We expect our model to perform similarly on both sets. If the performance on a test set will be poor comparing to a training set it would be an indicator for us that the model is overfitted and we have a "high variance" issue.

## Training set accuracy
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# train_loss, train_accuracy = model.evaluate(x_train_normalized, y_train)

print(f'Traning Loss: {train_loss}')
print(f'Traning Accuracy: {train_accuracy}')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# validation_loss, validation_accuracy = model.evaluate(x_test_normalized, y_test)

print('Validation Loss:', validation_loss)
print('Validation Accuracy:', validation_accuracy)

"""### Save the model
We will save the entire model to a HDF5 file. The .h5 extension of the file indicates that the model shuold be saved in Keras format as HDF5 file. To use this model on the front-end we will convert it (later in this notebook) to Javascript understandable format (tfjs_layers_model with .json and .bin files) using tensorflowjs_converter as it is specified in the main README.
"""

model_name = 'digit_recognition_cnn.h5'
model.save(model_name, save_format='h5')

loaded_model = tf.keras.models.load_model(model_name)

"""## Use the model (do predictions)

To use the model that we've just trained for digits recognition we need to call `predict()` method.
"""

predictions_one_hot = loaded_model.predict([x_test_normalized])

print('predictions_one_hot:', predictions_one_hot.shape)

print('predictions_one_hot:', predictions_one_hot.shape)

"""Each prediction consists of 10 probabilities (one for each number from 0 to 9). We need to pick the digit with the highest probability since this would be a digit that our model most confident with."""

# Predictions in form of one-hot vectors (arrays of probabilities)
pd.DataFrame(predictions_one_hot)

# Let's extract predictions with highest probabilites and detect what digits have been actually recognized.
predictions = np.argmax(predictions_one_hot, axis=1)
pd.DataFrame(predictions)

"""So our model is predicting that the first example from the test set is '7'."""

print(predictions[0])

"""Let's print the first image from a test set to see if model's prediciton is correct."""

plt.imshow(x_test_normalized[0].reshape((IMAGE_WIDTH, IMAGE_HEIGHT)), cmap=plt.cm.binary)
plt.show()

"""We see that our model made a correction prediction and it successfully recognized digit 7. Let's print some more test examples and correspondent predictions to see how model performs and where it does mistake"""

number_to_display = 198
num_cells = math.ceil(math.sqrt(number_to_display))
plt.figure(figsize=(15, 15))

for plot_index in range(number_to_display):
    predicted_label = predictions[plot_index]
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    color_map = 'Greens' if predicted_label == y_test[plot_index] else 'Reds'
    plt.subplot(num_cells, num_cells, plot_index + 1)
    plt.imshow(x_test_normalized[plot_index].reshape((IMAGE_WIDTH, IMAGE_HEIGHT)), cmap=color_map)
    plt.xlabel(predicted_label)

plt.subplots_adjust(hspace=1, wspace=0.5)
plt.show()

"""## Plotting a confusion matrix

[Confusion matrix] shows what numbers are recognized well by the model and what numbers the model usually confuses to recognize correctly. You may see that the model performs really well but sometimes (28 times out of 10000) it may confuse number `5` with `3` or number `2` with `3`.
"""

confusion_matrix = tf.math.confusion_matrix(y_test, predictions)
f, ax = plt.subplots(figsize=(9, 7))
sn.heatmap(
    confusion_matrix,
    annot=True,
    linewidths=.5,
    fmt="d",
    square=True,
    ax=ax
)

plt.show()

"""## Debugging the model with TensorBoard

[TensorBoard](https://www.tensorflow.org/tensorboard) is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more.
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir .logs/fit





